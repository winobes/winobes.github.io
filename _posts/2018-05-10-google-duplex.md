---
layout: post
title: What's the big deal about Google Duplex? 
---

<div class="message">
    Google is choosing when how we talk about AI ethics.
</div>

On Tuesday, Google demoed an update to its virtual assistant
that calls up businesses on your behalf to book appointments and make reservations.

In the demo, Duplex makes two calls, one to make a hair appointment, and another to reserve a table at a restaurant.
The reservation call is especially impressive because when it turns out the restaurant
will only take reservations for five or more people, Duplex politely takes no as an answer and knows to ask
if there's likely to be a wait.

It neither conversation did Duplex identify itself as non-human. It even goes so far as to introduce speech disfluencies (_ummms_ and _aahs_ and at least one sassy _mmm-hmmmmm_)
which [Google acknowledges](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html) are mostly about sounding more human.
I wouldn't mind that kind of thing if I know I'm talking to a computer, but it feels like an especially harsh betrayal
to be manipulated into believing an AI is human in that way.

Why did Google, with their armies of PR people, think it was ok to demo a product that misrepresents itself as human 
to unwitting service workers?

They _knew_ that this demo would cause a backlash, they just didn't know _how big_. 
And now they're getting to find out. So far we have:

* [Tech Crunch](https://techcrunch.com/2018/05/10/duplex-shows-google-failing-at-ethical-and-creative-ai-design/) - _Duplex shows Google failing ethical and creative AI design_
* [PC Mag](https://www.pcmag.com/commentary/361111/google-duplex-is-classist-heres-how-to-fix-it) - _Google Duplex is classist: Here's how to fix it_
* [The Verge](https://www.theverge.com/2018/5/9/17335710/google-duplex-phone-call-ai-assistant-service-industry) - _The selfishness of Google Duplex_
* [Slate](https://slate.com/technology/2018/05/google-duplex-can-make-phone-calls-for-you-but-it-should-have-to-identify-itself.html) - _Am I speaking to a human?_

As of a few hours ago [CNET is reporting](https://www.cnet.com/news/google-says-its-designing-duplex-with-disclosure-built-in/) that Google has already started "clarifying":

> We are designing this feature with disclosure built-in, and we'll make sure the system is appropriately identified. What we showed at I/O was an early technology demo, and we look forward to incorporating feedback as we develop this into a product.

But that doesn't really answer any of the _specific_ questions I want to have answered:

* does the assistant identify itself as non-human up front, or only when asked?
* can businesses opt out of receiving calls from Duplex?
* what action does Duplex take when the conversation fails? 

Instead, it looks like Google is stalling for time. They're _"incorporating feedback"_.
In other words, they're waiting to see what we decide is okay _and then_ they'll see what they can get away with. 

This kind of tactic can be used to shift the overton window on what behaviors we're ok with from our technology. 
Maybe that's already what's happening here, even.

One thing is for sure, there are plenty of other questions like _"Can computers misrepresent themselves as human?_ that AI companies want answered.
By presenting _their answer_ at a product demo, they get to set the terms of the discussion.
The tone of that demo ([here](https://www.youtube.com/watch?v=bd1mEm2Fy08) it is by the way) was clearly projecting _this is fine._
If that's where the conversation starts, maybe we end up closer to that position that we would have otherwise. 

That's clearly what Google's hoping, anyway.

But it doesn't have to be that way. 
In ethics classes, in technology journalism, in science fiction, in conversations with friends, on social media!
We can be talking about these questions. We can decide what we're ok with and what we're not.
And we don't have to accept that just because it seems like some thing already is some way that it always has to _be_ that way.
