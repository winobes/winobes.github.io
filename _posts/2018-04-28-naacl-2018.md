---
layout: post
title: NAACL papers I'm excited for
---

<div class="message">
    Based on their titles alone...
</div>

I just found out my company is sending me to NAACL! Thank you, Networked Insights 😁.
The [list of accepted NAACL papers](https://naacl2018.wordpress.com/2018/03/02/list-of-accepted-papers/) came out about a month ago and more and more papers are showing up on arXiv every day.

I took some time to go through the list, pick out some titles I thought sound interesting and see what I could find out about them. This is what I found.

# Corpora

**A corpus of non-native written English annotated for metaphor**  
_Beata Beigman Klebanov, Chee Wee (Ben) Leong and Michael Flor_

> Interesting to note that Beata Beigman Klemanov (what an iconic name by the way) works at ETS. It makes sense to me that the people who make those tests would be into this kind of annotation. It's also cool that it's non-native English  because you don't see a lot of datasets that are explicitly that and I don't know what percentage of English speakers are non-native but I'm sure it's a ***lot***\*. 
> 
> \* Holy #\*@%! L2 English speakers outnumber L1 speakers by a ration of 3 to 1! . Let's do more science on non-native English!

**A dataset of peer reviews (peerread): Collection, insights and NLP applications**  
_Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy and Roy Schwartz_

**Dear Sir or Madam, May I introduce the YAFC corpus: Corpus, benchmarks and metrics for formality style transfer**  
_Sudha Rao and Joel Tetreault_

> [Joel Tetreault](https://twitter.com/tetreault_nlp) works at [Grammerly](https://en.wikipedia.org/wiki/Grammarly) which makes software that does what they call _writing enhancement_ which surely includes spelling and grammar, but perhaps also some tooling around formality?. 
>
> [Sudha Rao](https://raosudha.weebly.com/) is an early PhD at UMD advised by Hal Daumé which makes me incredibly jealuous. 
> She also interned at Grammarly last summer which is surely where this work comes from. Also, her [thesis project](https://www.scribd.com/document/351169976/Teaching-Machines-to-Ask-Clarification-Questions) includes grounding which is a [favorite topic of mine](https://en.wikipedia.org/wiki/Grounding_in_communication). 

**LSDSCC: A large scale domain-specific conversational corpus for response generation with diversity oriented evaluation metrics**  
_Zhen Xu, Nan Jiang, Bingquan Liu, Wenge Rong, Bowen Wu, Baoxun Wang, Xiaolong Wang and Zhuoran Wang_


# Non-Monolithic Lexical Semantics

**A Laypeople study on terminology identification across domains and task definitions**  
_Anna Hätty and Sabine Schulte im Walde_


**ATTR2VEC: Jointly learning word and contextual attribute embeddings with factorization machines**  
_Fabio Petroni, Vassilis Plachouras, Timothy Nugent and Jochen L. Leidner_

> I'm always interested in work that tries to capture contextual information, especially when it's giving structure to word embeddings, which it sounds like this might. The authors have released their code on [github](https://github.com/thomsonreuters/attr2vec) but the readme is all business and I don't know enough about the topic to tell from the code what is going on, so I'll wait.

**Deep contextualized word representations**  
_Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer_

> I was a little skeptical of this work at first, I mean [hasn't](https://arxiv.org/pdf/1502.07257.pdf) this [been](http://www.aclweb.org/anthology/W16-5307) done [already](https://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf)? But I'm not sure any of those embeddings proved to be so useful for a broad variety of tasks, as this approach claims to. Also they've put both code up for both TensorFlow and PyTorch, which is awesome!

**MITTENS: an extension of glove for learning domain-specialized representations**  
_Nick Dingwall and Christopher Potts_

> So cute! Plus, Christopher Potts gets in on the GloVe game!

# Semantic Shift

**Deep neural models of semantic shift**  
_Alex Rosenfeld and Katrin Erk_

> It's interesting to think about using neural models to measure something like semantic shift which is an essentially social phenomenon since usually we just think of neural models as modeling just a single mind in some way. Not many clues about this one but it's on my list for sure.

**Diachronic usage relatedness (durel): A framework for the annotation of lexical semantic change**  
_Dominik Schlechtweg, Sabine Schulte im Walde and Stefanie Eckmann_

**Analogies in complex verb meaning shifts: The effect of affect in semantic similarity models**  
_Maximilian Köper and Sabine Schulte im Walde_

> I'm willing to listen to anyone who can tell me about the effect of affect.

# Computational Sociolinguistics

**Attentive interaction model: Modeling changes in view in argumentation**  
_Yohan Jo, Shivani Poddar, Byungsoo Jeon, Qinlan Shen, Carolyn Rose and Graham Neubig_

> When I read this title I knew immediately that they used [/r/changemyview](https://reddit.com/r/changemyview). An [earlier paper](https://arxiv.org/abs/1602.01103) by some of the Cornell computational sociolinguistics folks comes with a really nice dataset for it (with annotations)! They are so good about releasing data that's well documented, easy to use and not 404'd. [(arXiv)](https://arxiv.org/abs/1804.00065)

**Author commitment and social power: Automatic belief tagging to infer the social context of interactions**  
_Vinodkumar Prabhakaran and Owen Rambow_

> I took a look at the [author's thesis](http://www.cs.columbia.edu/nlp/theses/vinodkumar_prabhakaran.pdf). One of the main ideas is that you can predict social power based on their _level of belief_ in their utterances. "i.e., whether the participants are committed to the beliefs they express, non-committed to them, or express beliefs attributed to someone else". Sounds pretty interesting and I think that's what this paper is going to be about.

**Deconfounded lexicon induction for interpretable social science**  
_Reid Pryzant, Kelly Shen, Dan Jurafsky and Stefan Wagner_

> I'm very interested in this enigmatically titled paper. What is this lexicon and what does it mean for it to be _deconfounded_? In what way is it being _induced_? And we're doing... what with it? Interpreting social science!?! Fascinating. Unfortunately, all I could find is this [broken link](https://nlp.stanford.edu/projects/deconfounded-lexicon-induction/). 
> 
> There is also this [Github project](https://github.com/rpryzant/deconfounded_lexicon_induction) but but my mother told me to poke around in a code repository I know nothing about.

**Detecting linguistic characteristics of alzheimer’s dementia by interpreting neural models**  
_Sweta Karlekar, Tong Niu and Mohit Bansal_

> I put this one in the social science section even though it's really more cognitive science. I think because the kind of linguistic variation they're looking for and the measures they use for it are pretty relevant to the kinds of things that are being done in computational sociolinguistics.

**Inducing a lexicon of abusive words — A feature-based approach**  
_Michael Wiegand, Josef Ruppenhofer, Anna Schmidt and Clayton Greenberg_

**Si o no, que penses? Catalonian independence and linguistic identity on social media**  
_Ian Stewart, Yuval Pinter and Jacob Eisenstein_


# Others

**Colorless green recurrent networks dream hierarchically**  
_Kristina Gulordava, Marco Baroni, Tal Linzen, Piotr Bojanowski and Edouard Grave_

> Dad joke level title, but atually pretty descriptive. The authors want to see if RNNs learn abstract hierarchical syntactic structure. In other words, do they understand the ways in which words build into phrases and phrases build into sentences, and how they're connected once they do? 
> 
> To test this, they test if the RNN language model can get syntactic number agreement right in meaningless (but syntactically correct) sentences like "The colorless greed **ideas** I ate with the chair **sleep** furiously". In this example, if the RNN predicts _sleep_, which agrees with _ideas_ rather than _sleeps_, then that's evidence the RNN understands the syntactic structure.

**Deep dungeons and dragons: Learning character-action interactions from role-playing game transcripts**  
_Annie Louis and Charles Sutton_

> Well this is clearly awesome. I'm not sure exactly what it means for a character to interact with an action, but I'm excited to find out. Couldn't find so much as an abstract online, and the authors' past work doesn't have any big hints so we'll have to wait to find out.

**Fine-grained temporal orientation and its relationship with psycho-demographic correlates**  
_Sabyasachi Kamila, Mohammed Hasanuzzaman, Asif Ekbal, Pushpak Bhattacharyya and Andy Way_

> I'm putting this here so I remember to read the abstract later. I have no idea what this could be about but I want to know.

**Learning sentence representations over tree structures for target-dependent classification**  
Junwen Duan, Xiao Ding and Ting Liu

> Always on the lookout for syntactically aware sentence representations. Not sure what _target-dependent_ classification is.

**Unsupervised learning of sentence embeddings using compositional n-gram features**  
_Matteo Pagliardini, Prakhar Gupta and Martin Jaggi_

**What’s in a domain? learning domain-robust text representations using adversarial training**  
_Yitong Li, Trevor Cohn and Timothy Baldwin_

**Linguistic cues to deception and perceived deception in interview dialogues**  
_Sarah Ita Levitan, Angel Maredia and Julia Hirschberg_

> Yeesh, a lie detector?

**Multimodal emoji prediction**  
_Francesco Barbieri, Miguel Ballesteros, Francesco Ronzano and Horacio Saggion_

> I got excited because I thought they were modeling emoji usage as a secondary top-level mode of communication in messaging (akin to gesture), but actually it's multimodal because they use the photo, as well as comment text on Instagram posts to predict the emoji usage. Still cool though.

**Predicting helpful posts in open-ended discussion forums: A neural architecture**  
_Kishaloy Halder, Min-Yen Kan and Kazunari Sugiyama_

> This looks interesting, and I found a copy [here](https://pdfs.semanticscholar.org/5ff5/f771af73ae3a03aacac0a68b092740b3e9e8.pdf). I think it would be really frustrating to use a discussion forum that scores your post before anyone can see it though.

**Sentiment analysis: It’s complicated!**  
_Kian Kenyon-Dean, Eisha Ahmed, Scott Fujimoto, Jeremy Georges-Filteau, Christopher Glasz, Barleen Kaur, Auguste Lalande, Shruti Bhanderi, Robert Belfer, Nirmal Kanagasabai, Roman Sarrazingendron, Rohit Verma and Derek Ruths_

> Well I'm sure it *is* complicated when you have twelve different people weighing in!

